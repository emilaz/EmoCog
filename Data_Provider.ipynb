{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/miniconda3/envs/emocog/lib/python3.7/site-packages/hypertools/plot/__init__.py:10: UserWarning: Could not switch backend to TkAgg.  This may impact performance of the plotting functions.\n",
      "  warnings.warn('Could not switch backend to TkAgg.  This may impact performance of the plotting functions.')\n",
      "/home/emil/miniconda3/envs/emocog/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from FeatureRelated.feature_generator import Feature_generator\n",
    "from LabelRelated.label_generator import Label_generator \n",
    "from FeatureRelated.feature_data_holder import FeatDataHolder\n",
    "from LabelRelated.label_data_holder import LabelDataHolder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Vis import LabelVis, ClassificationVis\n",
    "from Util import DataUtils as dutil\n",
    "from Util import LabelUtils as lutil\n",
    "from Util import FeatureUtils as futil\n",
    "from Util import SyncUtils as sutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class that brings together feature and label side to provide all data needed for classification.\n",
    "Synchronizes the data.\n",
    "This class is not designed to be used elsewhere. Data should be generated and analysed here, then saved to file using the DataUtil class.\n",
    "\"\"\"\n",
    "class DataProvider:\n",
    "    \"\"\"\n",
    "    Init function. Start and end time are set for a time period I checked manually to be more or less okay.\n",
    "    Creates classes to hold label and feature data in memory and classes to create feats and labels.\n",
    "    Input: draw bool, if we want to visualize the happy/non-happy ratio et al.\n",
    "    \"\"\"\n",
    "    def __init__(self, draw = False):\n",
    "        self.is_loaded = False #bool to check whether the raw data has already been loaded into memory\n",
    "        self.draw = draw\n",
    "        self.all_days_df = None\n",
    "        \n",
    "    def _load_raws(self, patient, days): #TODO auf mehrere tage erweitern\n",
    "        if type(days) is int:\n",
    "            days = [days]\n",
    "        #this dataframe saves pat,day,st,end,the raw, non-standardized, non-PCA features and corresponding labels\n",
    "        all_days_df = pd.DataFrame(columns = ['Patient','Day','Start','End','BinnedData','BinnedLabels', 'GoodChans'], index=(0,len(days)-1))\n",
    "        for enum,day in enumerate(days):\n",
    "            path_ecog, path_vid = sutil.find_paths(patient,day)\n",
    "            realtime_start, realtime_end = sutil.find_start_and_end_time(path_vid) #output in secs from midnight\n",
    "            feat_data = FeatDataHolder(path_ecog,start=realtime_start, end=realtime_end)\n",
    "            label_data = LabelDataHolder(path_vid,0,realtime_end-realtime_start, col = 'Happy_predicted' )\n",
    "            all_days_df.loc[enum] = [patient, day,realtime_start,realtime_end,feat_data.get_bin_data(),label_data.get_pred_bin(), feat_data.chan_labels]\n",
    "        #self.annot_data = LabelDataHolder(path_vid,video_start,video_start+self.duration, col = 'annotated')\n",
    "        self.featuregen = Feature_generator(all_days_df)\n",
    "        self.lablegen = Label_generator(all_days_df)\n",
    "        #das hier erstmal nicht. spaeter zur analyse vielleicht wieder\n",
    "        #self.annotsgen = Label_generator(all_days_df,mask=self.featuregen.bad_indices['NaNs']) #this is for conf mat later\n",
    "        self.all_days_df = all_days_df\n",
    "        self.is_loaded = True\n",
    "        \n",
    "        \n",
    "\n",
    "    def reload_generators(self):\n",
    "        self.featuregen = Feature_generator(self.all_days_df)\n",
    "        self.lablegen = Label_generator(self.all_days_df)\n",
    "        \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Function to generate the feats and labels, given the input hyperparas\n",
    "    Input: Configs, i.e. Windowsize, sliding window, start and end (in s), train bool, variance to be explained, cutoff if classification.\n",
    "    Output: Features, Labels\n",
    "    \"\"\"\n",
    "    def generate_data(self,configs, train=True):\n",
    "        #train data\n",
    "        if 'expvar' not in configs.keys():\n",
    "            configs['expvar']=95\n",
    "        #check whether train or test data, set start and end sample accordingly\n",
    "        if train:\n",
    "            start=configs['s_sample']\n",
    "            end=configs['e_sample']\n",
    "        else:\n",
    "            start=configs['s_sample_ev']\n",
    "            end=configs['e_sample_ev']\n",
    "        x = self.featuregen.generate_features(wsize = configs['wsize'], start=start,end=end,expl_variance=configs['expvar'],train=train,sliding_window=configs['sliding'])\n",
    "        y,rat = self.lablegen.generate_labels(wsize = configs['wsize'], start=start,end=end, sliding_window=configs['sliding'])\n",
    "        #annots, _ = self.annotsgen.generate_labels(wsize=configs['wsize'], start=start,end=end, sliding_window=configs['sliding'])\n",
    "#         if self.draw:\n",
    "#             LabelVis.plot_happy_ratio(y,rat)\n",
    "#             LabelVis.plot_happy_ratio(annots,_)\n",
    "#             preds = y[~np.isnan(y)] ### this is for the confusion matrix between human annotations and openface labels\n",
    "#             annots = annots[~np.isnan(y)]\n",
    "#             preds = preds[~np.isnan(annots)]\n",
    "#             annots = annots[~np.isnan(annots)] ###\n",
    "#             ClassificationVis.conf_mat(preds, annots)\n",
    "        x_clean, y_clean = sutil.filter_data(x,y,self.featuregen.bad_indices)\n",
    "        return x_clean,y_clean\n",
    "\n",
    "    \n",
    "    def get_data(self, configs):\n",
    "        #if data already exists, simply reload\n",
    "        try:\n",
    "            x,y,x_ev, y_ev = dutil.load_data_from_file(configs)\n",
    "            print('Loading Data from File..done')\n",
    "        except FileNotFoundError: #file doesn't exist\n",
    "            print('Data not on file yet. Loading raw data into memory...')\n",
    "            if not self.is_loaded:\n",
    "                self._load_raws(configs['patient'],configs['days'])\n",
    "            print('And creating the data..')\n",
    "            x,y = self.generate_data(configs, train = True)\n",
    "            x_ev, y_ev = self.generate_data(configs, train = False)\n",
    "            print('Done. Saving to file for later use.')\n",
    "            dutil.save_data_to_file(x,y,x_ev, y_ev, configs)\n",
    "        #now do the cutoff\n",
    "        if 'cutoff' in configs.keys(): \n",
    "            cutoff = configs['cutoff']\n",
    "            print('Doing cutoff')\n",
    "            y = lutil.do_cutoff(y, cutoff)\n",
    "            y_ev = lutil.do_cutoff(y_ev, cutoff)\n",
    "        return x,y,x_ev,y_ev\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = DataProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data here, to parallelize\n",
    "patient = 'cb46fd46'\n",
    "days = [3,4,5]\n",
    "wsize = 100\n",
    "sliding = 25\n",
    "s_sample = 0\n",
    "e_sample = 120000\n",
    "s_sample_ev = 120000\n",
    "e_sample_ev = 150000\n",
    "expvar = 90\n",
    "configs =dict()\n",
    "configs['patient']=patient\n",
    "configs['days']=days\n",
    "configs['wsize']=wsize\n",
    "configs['sliding']=sliding\n",
    "configs['s_sample']=s_sample\n",
    "configs['e_sample']=e_sample\n",
    "configs['s_sample_ev']=s_sample_ev\n",
    "configs['e_sample_ev']=e_sample_ev\n",
    "configs['expvar'] = expvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current session goes into the next day, so edge case. Check if things went correctly.\n",
      "Current session goes into the next day, so edge case. Check if things went correctly.\n"
     ]
    }
   ],
   "source": [
    "#this should not be here, but since stuff isn't doing too well do anyways\n",
    "provider._load_raws(configs['patient'], configs['days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provider.reload_generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x,y,x_ev,y_ev = provider.get_data(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ev.shape, y_ev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data from File..done\n"
     ]
    }
   ],
   "source": [
    "h,m,h_ev,m_ev = provider.get_data(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_ev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# days=[3,4]\n",
    "# patient='cb46fd46'\n",
    "# all_days_df = pd.DataFrame(columns = ['Patient','Day','Start','End','BinnedData','BinnedLabels', 'GoodChans'], index=(0,len(days)-1))\n",
    "# for enum,day in enumerate(days):\n",
    "#     print('and go')\n",
    "#     path_ecog, path_vid = sutil.find_paths(patient,day)\n",
    "#     realtime_start, realtime_end = sutil.find_start_and_end_time(path_vid) #output in secs from midnight\n",
    "#     print(realtime_start, realtime_end)\n",
    "#     feat_data = FeatDataHolder(path_ecog,start=realtime_start, end=realtime_end)\n",
    "#     label_data = LabelDataHolder(path_vid,0,realtime_end-realtime_start+2000, col = 'Happy_predicted' )\n",
    "#     all_days_df.loc[enum] = [patient, day,realtime_start,realtime_end,feat_data.get_bin_data(),label_data.get_pred_bin(), feat_data.chan_labels]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuregena = Feature_generator(all_days_df)\n",
    "# lablegena = Label_generator(all_days_df)\n",
    "# print('get x')\n",
    "# x = featuregena.generate_features(wsize = 100, start=0,end=50000,expl_variance=90,train=True,sliding_window=10)\n",
    "# print('get y')\n",
    "# y,rat = lablegena.generate_labels(wsize = 100, start=0,end=50000, sliding_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to analyze the feature data a little bit\n",
    "realtime_start = 29413 #this is 8h, 10min and 13s into day 4\n",
    "video_start = realtime_start - 29344 # 29344 is the beginning of recordings of video data (see /home/emil/data/sync_data)\n",
    "duration = 37820+4500 #in seconds, of course\n",
    "holder = FeatDataHolder('/nas/ecog_project/derived/processed_ecog/cb46fd46/full_day_ecog/cb46fd46_fullday_4.h5',start=realtime_start, duration=duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = Feature_generator(holder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = men._calc_features(0,40000,train=True,wsize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bla_stand = futil.standardize(bla,np.std(bla,axis=1), np.mean(bla,axis=1))\n",
    "high_freq_bins = bla[7::8,:]\n",
    "\n",
    "#here, we do the visual analysis of the different bins\n",
    "#For the 120 frequency bin, plot distribution of psd over time, over channels\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(high_freq_bins.flatten(), kde = False, rug=True, rug_kws={'alpha':.2,'color':'gray'}, hist_kws={'alpha':1})\n",
    "plt.title('Histogram of High-Freq bin across channels and time')\n",
    "plt.xlabel('PSD')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "\n",
    "#now, do the violin plots to check for bad channels\n",
    "#first, we create a dictionary out of the individual channels\n",
    "dff = pd.DataFrame(columns=['ChanLabel','PSD Values'])\n",
    "for enum,chan in enumerate(holder.chan_labels):\n",
    "    dff.loc[enum]=(chan,high_freq_bins[enum])\n",
    "dff = dff.explode('PSD Values').reset_index(drop=True)\n",
    "dff['PSD Values'] = dff['PSD Values'].astype('float')\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(x=dff['ChanLabel'],y=dff['PSD Values'])\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the filtering from before\n",
    "artis = futil.detect_artifacts(bla)\n",
    "bla_clean = futil.remove_artifacts(bla,artis)\n",
    "\n",
    "\n",
    "print(bla_clean.shape)\n",
    "print(bla.shape)\n",
    "\n",
    "#bla_stand = futil.standardize(bla_clean,np.std(bla_clean,axis=1), np.mean(bla_clean,axis=1))\n",
    "bla_stand = bla_clean\n",
    "high_freq_bins = bla_stand[7::8,:]\n",
    "\n",
    "#here, we do the visual analysis of the different bins\n",
    "#For the 120 frequency bin, plot distribution of psd over time, over channels\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.distplot(high_freq_bins.flatten(), kde = False, rug=True, rug_kws={'alpha':.2,'color':'gray'}, hist_kws={'alpha':1})\n",
    "plt.title('Histogram of High-Freq bin across channels and time')\n",
    "plt.xlabel('PSD')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "\n",
    "#now, do the violin plots to check for bad channels\n",
    "#first, we create a dictionary out of the individual channels\n",
    "dff = pd.DataFrame(columns=['ChanLabel','PSD Values'])\n",
    "for enum,chan in enumerate(holder.chan_labels):\n",
    "    dff.loc[enum]=(chan,high_freq_bins[enum])\n",
    "dff = dff.explode('PSD Values').reset_index(drop=True)\n",
    "dff['PSD Values'] = dff['PSD Values'].astype('float')\n",
    "\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.violinplot(x=dff['ChanLabel'],y=dff['PSD Values'])\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataProvider(draw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('/home/emil/OpenMindv2/data') if f.endswith('hdf')]\n",
    "provider = DataProvider()\n",
    "for file in files:\n",
    "    print('File',file)\n",
    "    for cut in [.6]:\n",
    "        print('Cut=',cut)\n",
    "        configs = dutil.generate_configs_from_file(file, cut)\n",
    "        x,y,x_ev,y_ev = provider.get_data(configs)\n",
    "        print(np.unique(y,return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #generate data here, to parallelize\n",
    "# wsize = 5\n",
    "# s_sample = 0\n",
    "# e_sample = 30000\n",
    "# s_sample_ev = 30000\n",
    "# e_sample_ev = 35000\n",
    "# expvar = 90\n",
    "# configs =dict()\n",
    "# configs['wsize']=wsize\n",
    "# configs['s_sample']=s_sample\n",
    "# configs['e_sample']=e_sample\n",
    "# configs['s_sample_ev']=s_sample_ev\n",
    "# configs['e_sample_ev']=e_sample_ev\n",
    "# configs['expvar'] = expvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for wsize in [10,30,50,100]:\n",
    "#     configs['wsize'] = wsize\n",
    "#     if wsize == 10:\n",
    "#         for sliding in [False, 3, 5]:\n",
    "#             configs['sliding'] = sliding\n",
    "#             x,y, x_ev, y_ev = data.get_data(configs)\n",
    "#     if wsize == 30: \n",
    "#         for sliding in [False,5,15]:\n",
    "#             configs['sliding'] = sliding\n",
    "#             x,y, x_ev, y_ev = data.get_data(configs)\n",
    "#     if wsize == 50: \n",
    "#         for sliding in [False,15,25,35]:\n",
    "#             configs['sliding'] = sliding\n",
    "#             x,y, x_ev, y_ev = data.get_data(configs)\n",
    "#     if wsize == 100:\n",
    "#         for sliding in [False,25,50,75]:\n",
    "#             configs['sliding'] = sliding\n",
    "#             x,y, x_ev, y_ev = data.get_data(configs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x.shape\n",
    "\n",
    "# np.median(abs(x))\n",
    "\n",
    "# plt.plot(x[:,0])\n",
    "\n",
    "# dutil.save_data_to_file(x,y,x_ev,y_ev,configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this is to find cutoff\n",
    "\n",
    "# bla = DataProvider(draw=True)\n",
    "# ma = DataProvider(draw=True, col ='annotated')\n",
    "\n",
    "# y = bla.get_data(sliding=10)\n",
    "# my = ma.get_data(sliding=10)\n",
    "\n",
    "# thresh = .3\n",
    "# for thresh in [.2]:\n",
    "#     y_class = y.copy()\n",
    "#     my_class = my.copy()\n",
    "#     y_class_nans = np.isnan(y_class)\n",
    "#     my_class_nans = np.isnan(my_class)\n",
    "#     print(y_class_nans)\n",
    "#     y_class[y_class<thresh]=0\n",
    "#     y_class[y_class>0]=1\n",
    "#     y_class[y_class_nans]=np.nan\n",
    "#     my_class[my_class<thresh]=0\n",
    "#     my_class[my_class>0]=1\n",
    "#     my_class[my_class_nans]=np.nan\n",
    "\n",
    "#     LabelVis.plot_nan_ratio(my_class)\n",
    "#     LabelVis.plot_nan_ratio(y_class)\n",
    "\n",
    "#     ynonan =y_class[~my_class_nans]\n",
    "#     mynonan = my_class[~my_class_nans]\n",
    "#     mynonan = mynonan[~np.isnan(ynonan)]\n",
    "#     ynonan = ynonan[~np.isnan(ynonan)]\n",
    "\n",
    "#     ClassificationVis.conf_mat(mynonan,ynonan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P3 emocog",
   "language": "python",
   "name": "emocog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
