{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import warnings\n",
    "import util.feature_utils as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to generate features. Uses preprocessed data held in memory by FeatDataHolder class.\n",
    "\"\"\"\n",
    "class Feature_generator:\n",
    "    \n",
    "    \"\"\"\n",
    "    Init function.\n",
    "    \"\"\"\n",
    "    def __init__(self,df):\n",
    "        #sampling frequency and last sample taken\n",
    "        self.sfreq = 500 #will always be, hopefully lol\n",
    "        #TODO CHange this back to use the one from featureutils!!!!\n",
    "        self.df = util.filter_common_channels(df)\n",
    "        self.bad_indices = None #this needs to be passed on to the label side. Will include bad indices found during calculation\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    Function needed for calculating the features. Central piece on the feature side. Works as follows:\n",
    "    columns: channels, for each channel the 150 frequencies (0-150Hz) (hece freq*cha length), binned logarithmically\n",
    "    Rows: Time steps, defined by sliding window+window size\n",
    "    resulting matrix is 2D, Time Stepsx(Freq*Channels)\n",
    "    In case of generating train data, this function also saves mean and stddev for standardization purpose.\n",
    "    Input: Start and end time (in secs), bool for whether train data or not (for PCA), window size and sliding window in sec\n",
    "    Output: Standardized, binned data.\n",
    "    \"\"\"\n",
    "    def _generate_features_single_day(self,data, wsize = 100, sliding_window=False):\n",
    "        bads = []\n",
    "        time_it = 0\n",
    "        mat = None\n",
    "        idx = 0\n",
    "        while True:\n",
    "            stop = time_it + wsize\n",
    "            if stop > data.shape[1]:\n",
    "                print('This day gave us {} seconds worth of data'.format(data.shape[1]-1))\n",
    "                break\n",
    "            #Note that each column is exactly one second.\n",
    "            #get data in range of ALL channels\n",
    "            curr_data = data[:,time_it:stop,:].reshape(data.shape[0],-1)\n",
    "            #welch method \n",
    "            fr,psd = signal.welch(curr_data,self.sfreq,nperseg=250)\n",
    "            \n",
    "            #if there are nans in the psd, something's off. throw away, save index, continue\n",
    "            if np.isnan(psd).any():\n",
    "                bads +=[idx] #current index baad\n",
    "                if (sliding_window):\n",
    "                    time_it += sliding_window\n",
    "                else:\n",
    "                    time_it += wsize\n",
    "                idx+=1\n",
    "                continue\n",
    "            \n",
    "            fr_bin,psd_bin = util.bin_psd(fr,psd)\n",
    "            idx+=1\n",
    "            if mat is None:\n",
    "                #first time. create first column, flatten w/o argument is row major \n",
    "                mat = psd_bin.flatten()\n",
    "            else:\n",
    "                #after, add column for each time step\n",
    "                mat = np.column_stack((mat,psd_bin.flatten()))\n",
    "            #sliding window?\n",
    "            if (sliding_window):\n",
    "                time_it += sliding_window\n",
    "            else:\n",
    "                time_it += wsize\n",
    "        return mat, bads #we do the standardization after the filtering\n",
    "    \n",
    "    \n",
    "    def generate_features(self, wsize = 100, sliding_window=False):\n",
    "        #here, check how many days we need for the requested datasize\n",
    "        curr_data = None\n",
    "        for day in self.df['Day']:\n",
    "            print('Day', day)\n",
    "            data = self.df[self.df['Day']==day].BinnedData.values[0]\n",
    "            mat, bad = self._generate_features_single_day(data, wsize, sliding_window)\n",
    "            if curr_data is None:\n",
    "                curr_data = mat\n",
    "                self.bad_indices = np.array(bad)\n",
    "            else:\n",
    "                self.bad_indices = np.append(self.bad_indices,np.array(bad)+len(self.bad_indices)+curr_data.shape[1])\n",
    "                if mat is not None:\n",
    "                    curr_data = np.append(curr_data,mat,axis=1)\n",
    "        return curr_data\n",
    "            \n",
    "\n",
    "    \"\"\"\n",
    "    Function to return the bad indices found by filtering. Important: First filter out the nan indices, then the artifacts!\n",
    "    Order is important\n",
    "    Output: Dictionary of bad data points.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_bad_indices(self):\n",
    "        return self.bad_indices\n",
    "        \n",
    "\n",
    "\n",
    "# def filter_common_channels(common_df):\n",
    "#     good_idx = util.find_common_channels(common_df[['Day','GoodChans']])\n",
    "#     for idx,day in enumerate(good_idx['Day']):\n",
    "#         good = good_idx.loc[good_idx['Day']==day,'CommonChans'][idx]\n",
    "#         new_data = common_df[common_df['Day']==day]['BinnedData'][idx][good,:]\n",
    "#         common_df.loc[common_df['Day']==day,'BinnedData'] = [new_data]\n",
    "#         spraa = common_df.loc[common_df['Day']==day,'GoodChans'][idx][good]\n",
    "#         common_df.loc[common_df['Day']==day,'GoodChans'] = [spraa]\n",
    "#     return common_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days = [3,4]\n",
    "# all_days_df = pd.DataFrame(columns = ['Patient','Day','BinnedData','BinnedLabels', 'GoodChans'], index=range(len(days)))\n",
    "# for enum,day in enumerate(days):\n",
    "#     print(day,'this day')\n",
    "#     ####for testing\n",
    "#     labels = np.arange(day,10)[:,None]\n",
    "#     features = np.tile(np.arange(day,10)[None,:],(3,1)).astype('float')\n",
    "#     features[:,2]=np.nan\n",
    "#     print(features)\n",
    "#     good_chans = np.array(['yes'+str(day),'mes','tes'])\n",
    "#     curr_ret = ['test', day, features,labels,good_chans]      \n",
    "#     all_days_df.loc[enum] = curr_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = Feature_generator(all_days_df)\n",
    "# gen.generate_features(wsize =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### A WHOLE LOT OF PLOTTING FUNCTIONS\n",
    "\n",
    "\n",
    "# wut=pecog.generate_features(0,12500,expl_variance=90)\n",
    "\n",
    "# # print(wut.shape)\n",
    "\n",
    "\n",
    "# pecog.vis_pc()\n",
    "\n",
    "# pecog.curr_data.shape\n",
    "\n",
    "\n",
    "# lel=pecog.curr_data.T\n",
    "# med=np.median(lel.reshape(-1,8,lel.shape[1]),axis=0)\n",
    "# men=np.mean(lel.reshape(-1,8,lel.shape[1]),axis=0)\n",
    "# print(lel.shape)\n",
    "# for i in range(8):\n",
    "#     plt.plot(lel[8*8+i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation results')\n",
    "# plt.show()\n",
    "# for i in range(8):\n",
    "#     plt.plot(men[i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# #plt.yscale('log')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation, Mean over Channels - Standardized ')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(8):\n",
    "#     plt.plot(med[i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# #plt.yscale('log')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation, Median over Channels - Standardized')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# lel=pecog.temp_mat.T\n",
    "# med=np.median(lel.reshape(-1,8,lel.shape[1]),axis=0)\n",
    "# men=np.mean(lel.reshape(-1,8,lel.shape[1]),axis=0)\n",
    "# print(lel.shape)\n",
    "# for i in range(8):\n",
    "#     plt.plot(lel[8*8+i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation results')\n",
    "# plt.show()\n",
    "# for i in range(8):\n",
    "#     plt.plot(men[i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# #plt.yscale('log')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation, Mean over Channels ')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(8):\n",
    "#     plt.plot(med[i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# #plt.yscale('log')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation, Median over Channels')\n",
    "# plt.show()\n",
    "\n",
    "# f=h5py.File('/data2/users/stepeter/Preprocessing/processed_cb46fd46_4.h5')\n",
    "\n",
    "# sprr=f['dataset'][()]\n",
    "\n",
    "# for i in range(8,14):\n",
    "#     plt.plot(sprr[i,125*100*500:127*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.show()\n",
    "# for i in range(10,12):\n",
    "#     plt.plot(pecog.data[i,125*100*500:127*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(8,14):\n",
    "#     plt.plot(sprr[i,10*100*500:12*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.title('One Patient, Sample Channels')\n",
    "\n",
    "# for i in range(25,28):\n",
    "#     plt.plot(pecog.data[i,35*100*500:40*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(25,28):\n",
    "#     plt.plot(pecog.data[i,38*100*500:40*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(25,28):\n",
    "#     plt.plot(pecog.data[i,38*100*500:50*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.show()\n",
    "\n",
    "# np.argmax(lel.reshape(-1,8,lel.shape[1])[:,1,34])\n",
    "\n",
    "# pecog.generate_features()\n",
    "\n",
    "# ttest=np.abs(pecog.pca.transform(np.eye(pecog.curr_data.shape[1])))\n",
    "\n",
    "# ttest_sum[:16]\n",
    "\n",
    "# ttest.shape\n",
    "\n",
    "# #how much did each bin contribute?\n",
    "# ttest_sum=ttest.sum(axis=1)\n",
    "# ttest_sum=ttest[:,0]\n",
    "# ttest_shaped=ttest_sum.reshape(pecog.data.shape[0],-1)\n",
    "# cont_bins=ttest_shaped.sum(axis=0)\n",
    "# cont_elecs=ttest_shaped.sum(axis=1)\n",
    "# plt.figure(figsize=(7,3))\n",
    "# plt.bar(np.arange(len(cont_bins)),cont_bins)\n",
    "# ticks=['[0,1]','(1,2]','(2,4]','(4,8]','(8,16]','(16,32]','(32,64]','(64,150]']\n",
    "# plt.xticks(np.arange(len(cont_bins)),ticks)\n",
    "# plt.title('Contributions of Bins to PD 0')\n",
    "# plt.xlabel('Bins')\n",
    "# plt.ylabel('Absolute Contribution')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(3,15))\n",
    "# plt.barh(np.arange(len(cont_elecs)),cont_elecs)\n",
    "# plt.yticks(np.arange(len(cont_elecs)),list(pecog.chan_labels))\n",
    "# plt.ylabel('Region')\n",
    "# plt.xlabel('Absolute Contribution')\n",
    "# plt.title('Contributions of Chans to PD')\n",
    "\n",
    "# mni_file=pd.read_excel('/data2/users/stepeter/mni_coords/cb46fd46/cb46fd46_MNI_atlasRegions.xlsx')\n",
    "# for en,i in enumerate(pecog.chan_labels):\n",
    "#     print(en)\n",
    "#     if( sum(mni_file['Electrode'].isin([i]))==0):\n",
    "#         print(i)\n",
    "#         print(en)\n",
    "        \n",
    "\n",
    "# print(mni_file['Electrode'].loc[7])\n",
    "\n",
    "# pecog.chan_labels.shape\n",
    "\n",
    "# pc1=np.sum(np.abs(ttest[:,1].reshape(-1,8)),axis=1)\n",
    "# pc2=np.sum(np.abs(ttest[:,2].reshape(-1,8)),axis=1)\n",
    "# pc3=np.sum(np.abs(ttest[:,3].reshape(-1,8)),axis=1)\n",
    "# pc4=np.sum(np.abs(ttest[:,4].reshape(-1,8)),axis=1)\n",
    "\n",
    "# mni_coords_fullfile='/data2/users/stepeter/mni_coords/cb46fd46/cb46fd46_MNI_atlasRegions.xlsx'\n",
    "# plot_ecog_electrodes_mni_from_file_and_labels(mni_coords_fullfile,pecog.chan_labels,num_grid_chans=64, colors=cont_elecs[:-1])\n",
    "\n",
    "# plot_ecog_electrodes_mni_from_file_and_labels(mni_coords_fullfile,pecog.chan_labels,num_grid_chans=64, colors=pc1[:-1])\n",
    "# plot_ecog_electrodes_mni_from_file_and_labels(mni_coords_fullfile,pecog.chan_labels,num_grid_chans=64, colors=pc2[:-1])\n",
    "# plot_ecog_electrodes_mni_from_file_and_labels(mni_coords_fullfile,pecog.chan_labels,num_grid_chans=64, colors=pc3[:-1])\n",
    "# plot_ecog_electrodes_mni_from_file_and_labels(mni_coords_fullfile,pecog.chan_labels,num_grid_chans=64, colors=pc4[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
