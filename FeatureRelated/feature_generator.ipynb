{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "import warnings\n",
    "from Util import FeatureUtils as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to generate features. Uses preprocessed data held in memory by FeatDataHolder class.\n",
    "\"\"\"\n",
    "class Feature_generator:\n",
    "    \n",
    "    \"\"\"\n",
    "    Init function.\n",
    "    \"\"\"\n",
    "    def __init__(self,df):\n",
    "        #sampling frequency and last sample taken\n",
    "        self.sfreq = 500 #will always be, hopefully lol\n",
    "        self.df = util.filter_common_channels(df)\n",
    "        self.pca = None\n",
    "        self.std = None #these parameters are used for standardization.\n",
    "        self.mean = None # Use same parameter and apply to eval/test data.\n",
    "        self.std_lim = None #used for artifact detection, also on eval set.\n",
    "        self.std_med = None\n",
    "        self.bad_indices = dict() #this needs to be passed on to the label side. Will include bad indices found during calculation and artifacts found later\n",
    "\n",
    "    \"\"\"\n",
    "    Function needed for calculating the features. Central piece on the feature side. Works as follows:\n",
    "    columns: channels, for each channel the 150 frequencies (0-150Hz) (hece freq*cha length), binned logarithmically\n",
    "    Rows: Time steps, defined by sliding window+window size\n",
    "    resulting matrix is 2D, Time Stepsx(Freq*Channels)\n",
    "    In case of generating train data, this function also saves mean and stddev for standardization purpose.\n",
    "    Input: Start and end time (in secs), bool for whether train data or not (for PCA), window size and sliding window in sec\n",
    "    Output: Standardized, binned data.\n",
    "    \"\"\"\n",
    "    def _calc_features(self,data, time_sta,time_stp, wsize = 100, sliding_window=False):\n",
    "        bads = []\n",
    "        time_it = time_sta\n",
    "        mat = None\n",
    "        idx = 0\n",
    "        print('from {} to {}'.format(time_sta,time_stp))\n",
    "        while True:\n",
    "            stop = time_it + wsize\n",
    "            if stop >= data.shape[1]-1:\n",
    "                print('we went here, why? shape is', data.shape[1]-1)\n",
    "                break\n",
    "            #Note that each column is exactly one second.\n",
    "            #get data in range of ALL channels\n",
    "            curr_data = data[:,time_it:stop,:].reshape(data.shape[0],-1)\n",
    "            \n",
    "            #welch method \n",
    "            fr,psd = signal.welch(curr_data,self.sfreq,nperseg=250)\n",
    "            \n",
    "            #if there are nans in the psd, something's off. throw away, save index, continue\n",
    "            if np.isnan(psd).any():\n",
    "                bads +=[idx] #current index baad\n",
    "                if (sliding_window):\n",
    "                    time_it += sliding_window\n",
    "                else:\n",
    "                    time_it += wsize\n",
    "                if time_it + wsize >= time_stp+1:\n",
    "                    break\n",
    "                idx+=1\n",
    "                continue\n",
    "                  \n",
    "            fr_bin,psd_bin = util.bin_psd(fr,psd)\n",
    "            idx+=1\n",
    "            if mat is None:\n",
    "                self.fr_bin = fr_bin\n",
    "                #first time. create first column, flatten w/o argument is row major \n",
    "                mat = psd_bin.flatten()\n",
    "            else:\n",
    "                #after, add column for each time step\n",
    "                mat = np.column_stack((mat,psd_bin.flatten()))\n",
    "            #sliding window?\n",
    "            if (sliding_window):\n",
    "                time_it += sliding_window\n",
    "            else:\n",
    "                time_it += wsize\n",
    "            if time_it + wsize >= time_stp+1:\n",
    "                break\n",
    "        return mat, bads #we do the standardization after the filtering\n",
    "    \n",
    "    \n",
    "    def _calc_features_over_days(self,time_sta,time_stp, wsize = 100, sliding_window=False):\n",
    "        #here, check how many days we need for the requested datasize\n",
    "        duration = time_stp - time_sta\n",
    "        time_passed = 0\n",
    "        curr_data = None\n",
    "        idx = 0\n",
    "        while duration>time_passed+wsize: #if not a single additional window would fit, break\n",
    "            print('jo schau')\n",
    "            try:\n",
    "                day = self.df['Day'].loc[idx]\n",
    "                print('Day No {}'.format(day))\n",
    "            except KeyError:\n",
    "                print(\"Not enough data loaded into memory for this request.\")\n",
    "                return curr_data\n",
    "            print('time start is {}, and the end of the first day is{}'.format(time_sta, self.df['End'].loc[idx]))\n",
    "            if time_sta >= self.df['End'].loc[idx]-self.df['Start'].loc[idx]: #if startsample is after duration of data of first day, go to next day, change stuff\n",
    "                passed_not_used = self.df['End'].loc[idx]-self.df['Start'].loc[idx]\n",
    "                time_sta -= passed_not_used #how much do we have to reduce time_sta?\n",
    "                time_stp -= passed_not_used\n",
    "                print('jo soviel vergangen{}, so sind die nun {},{}'.format(passed_not_used,time_sta,time_stp))\n",
    "                continue\n",
    "            data = self.df['BinnedData'].loc[idx]\n",
    "            mat, bad = self._calc_features(data,time_sta,time_sta+duration-time_passed, wsize, sliding_window)\n",
    "            if idx == 0:\n",
    "                self.bad_indices['NaNs'] = np.array(bad)\n",
    "                curr_data = mat\n",
    "                print('jo hier war ich jetzt drin.')\n",
    "            else:\n",
    "                self.bad_indices['NaNs'] = np.append(self.bad_indices['NaNs'],np.array(bad)+len(self.bad_indices['NaNs'])+curr_data.shape[1])\n",
    "                print(self.bad_indices['NaNs'], 'this is how the nan indices look after adding some on the next day')\n",
    "                curr_data = np.append(curr_data,mat,axis=1)\n",
    "            idx +=1\n",
    "            print('jo das ist die laenge', len(self.bad_indices['NaNs']), 'das die andere', curr_data.shape )\n",
    "            #calculate how many secs have passed\n",
    "            if sliding_window:\n",
    "                time_passed = wsize+sliding_window*(curr_data.shape[1] + len(self.bad_indices['NaNs'])-1)\n",
    "            else:\n",
    "                time_passed = wsize*(curr_data.shape[1]+ len(self.bad_indices['NaNs']))\n",
    "            print(time_passed, 'jo stimmt das hier mit der time passed?')\n",
    "            time_sta = 0 #for the next day, in case the initial starting time wasn't zero\n",
    "        return curr_data\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    Sets up PCA parameters in case of training, otherwise just transforms\n",
    "    Input: Data, train bool, amount of variance one wants to be explained (automatically calculates no. of PD needed)\n",
    "    Output: Data in PC space.\n",
    "    \"\"\"\n",
    "    def _setup_PCA(self,curr_data,train,expl_variance):\n",
    "        if not train and self.pca is None:\n",
    "            raise ValueError('Train set has to be generated first, otherwise no principal axis available for data trafo.')\n",
    "        if train:\n",
    "            print('Setting up PCA on current data range...')\n",
    "            no_comps=util.get_no_comps(curr_data,expl_variance)\n",
    "            self.pca=PCA(n_components=no_comps)\n",
    "            self.pca.fit(curr_data)\n",
    "        return self.pca.transform(curr_data)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function that actually generates the features.\n",
    "    Input: Start and end time (in secs), windowsize, sliding window (in s), train bool, variance to be explained by PCA.\n",
    "    Output: Features\n",
    "    \"\"\"\n",
    "    def generate_features(self,start=0,end=None, wsize=100, sliding_window=False, train=True,expl_variance=85):\n",
    "        curr_data = self._calc_features_over_days(start,end,wsize,sliding_window)\n",
    "        #from here on, days don't matter anymore. We have a chunk of data, which is nice\n",
    "        if train:\n",
    "            self.bad_indices['Artifacts'], self.std_lim, self.std_med = util.detect_artifacts(curr_data) #JO HIER TRAIN MITGEEBEN\n",
    "        else:\n",
    "            self.bad_indices['Artifacts'], _,_ = util.detect_artifacts(curr_data, self.std_lim,self.std_med) #JO HIER TRAIN MITGEEBEN        \n",
    "        good_data = util.remove_artifacts(curr_data, self.bad_indices['Artifacts'])\n",
    "        if train: #if it's train data, then get its mean and std for standardization\n",
    "            self.std = np.std(good_data,axis=1)\n",
    "            self.mean = np.mean(good_data,axis=1)\n",
    "        data_scal = util.standardize(good_data,self.std,self.mean)\n",
    "        princ_components=self._setup_PCA(data_scal.T,train=train,expl_variance=expl_variance)\n",
    "        return princ_components\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to return the bad indices found by filtering. Important: First filter out the nan indices, then the artifacts!\n",
    "    Order is important\n",
    "    Output: Dictionary of bad data points.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_bad_indices(self):\n",
    "        return self.bad_indices\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pecog=Feature_generator('/data2/users/stepeter/Preprocessing/processed_cb46fd46_4.h5',prefiltered=False,wsize=100)\n",
    "# mecog = Feature_generator('/nas/ecog_project/derived/processed_ecog/cb46fd46/full_day_ecog/cb46fd46_fullday_4.h5', prefiltered =False, wsize =100)\n",
    "# for p in range(pecog.pca.n_components):\n",
    "#     plt.plot(wut[:,p])\n",
    "# plt.xlabel('Time (in w_size)')\n",
    "# plt.ylabel('PC Value')\n",
    "# plt.title('First %d principal components' % pecog.pca.n_components)\n",
    "# plt.show()\n",
    "\n",
    "# pecog.vis_raw_data(0,30000,range(20))\n",
    "\n",
    "# #pecog.vis_raw_data(idx[0]-5,idx[1]+5)\n",
    "# #pecog.vbis_raw_data(0,idx[1]+400)\n",
    "# pecog.vis_welch_data(0,30000)\n",
    "\n",
    "\n",
    "\n",
    "# good_data=pecog.calc_data_mat(idx[1]+100,idx[1]+400)\n",
    "# pecog.pca.fit(good_data)\n",
    "# good_data_trafo=pecog.pca.transform(good_data)\n",
    "# print(good_data_trafo.shape)\n",
    "# print(good_data.shape)\n",
    "\n",
    "\n",
    "# print(good_data.shape)\n",
    "# print(good_data_trafo.shape)\n",
    "\n",
    "# comps=pecog.pca.components_\n",
    "# print(pecog.raw.info['ch_names'][28])\n",
    "# print(comps.shape)\n",
    "# comps=comps.reshape((127,-1,2))\n",
    "# print(np.argmax(comps[:,:,1],axis=1))\n",
    "# #plt.plot(comps[5:,5:,0].T)\n",
    "# plt.plot(comps[:,:,1].T)\n",
    "# plt.ylim(-0.005,0.005)\n",
    "\n",
    "# print(len(pecog.raw.info['chs']))\n",
    "\n",
    "# #print(data_trafo)\n",
    "# plt.plot(good_data_trafo[:,0])\n",
    "# plt.plot(good_data_trafo[:,1])\n",
    "# #plt.xlim(-0.00001,0.00001)\n",
    "# #plt.ylim(-0.00001,0.00001)\n",
    "\n",
    "# #max(data_trafo[:,1])-min(data_trafo[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### A WHOLE LOT OF PLOTTING FUNCTIONS\n",
    "\n",
    "\n",
    "# wut=pecog.generate_features(0,12500,expl_variance=90)\n",
    "\n",
    "# # print(wut.shape)\n",
    "\n",
    "\n",
    "# pecog.vis_pc()\n",
    "\n",
    "# pecog.curr_data.shape\n",
    "\n",
    "\n",
    "# lel=pecog.curr_data.T\n",
    "# med=np.median(lel.reshape(-1,8,lel.shape[1]),axis=0)\n",
    "# men=np.mean(lel.reshape(-1,8,lel.shape[1]),axis=0)\n",
    "# print(lel.shape)\n",
    "# for i in range(8):\n",
    "#     plt.plot(lel[8*8+i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation results')\n",
    "# plt.show()\n",
    "# for i in range(8):\n",
    "#     plt.plot(men[i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# #plt.yscale('log')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation, Mean over Channels - Standardized ')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(8):\n",
    "#     plt.plot(med[i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# #plt.yscale('log')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation, Median over Channels - Standardized')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# lel=pecog.temp_mat.T\n",
    "# med=np.median(lel.reshape(-1,8,lel.shape[1]),axis=0)\n",
    "# men=np.mean(lel.reshape(-1,8,lel.shape[1]),axis=0)\n",
    "# print(lel.shape)\n",
    "# for i in range(8):\n",
    "#     plt.plot(lel[8*8+i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation results')\n",
    "# plt.show()\n",
    "# for i in range(8):\n",
    "#     plt.plot(men[i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# #plt.yscale('log')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation, Mean over Channels ')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(8):\n",
    "#     plt.plot(med[i,:], label='Bin %d' %i)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time Window')\n",
    "# #plt.yscale('log')\n",
    "# plt.ylabel('PSD')\n",
    "# plt.title('Welch Transformation, Median over Channels')\n",
    "# plt.show()\n",
    "\n",
    "# f=h5py.File('/data2/users/stepeter/Preprocessing/processed_cb46fd46_4.h5')\n",
    "\n",
    "# sprr=f['dataset'][()]\n",
    "\n",
    "# for i in range(8,14):\n",
    "#     plt.plot(sprr[i,125*100*500:127*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.show()\n",
    "# for i in range(10,12):\n",
    "#     plt.plot(pecog.data[i,125*100*500:127*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(8,14):\n",
    "#     plt.plot(sprr[i,10*100*500:12*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.title('One Patient, Sample Channels')\n",
    "\n",
    "# for i in range(25,28):\n",
    "#     plt.plot(pecog.data[i,35*100*500:40*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(25,28):\n",
    "#     plt.plot(pecog.data[i,38*100*500:40*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(25,28):\n",
    "#     plt.plot(pecog.data[i,38*100*500:50*100*500])\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('uV')\n",
    "# plt.show()\n",
    "\n",
    "# np.argmax(lel.reshape(-1,8,lel.shape[1])[:,1,34])\n",
    "\n",
    "# pecog.generate_features()\n",
    "\n",
    "# ttest=np.abs(pecog.pca.transform(np.eye(pecog.curr_data.shape[1])))\n",
    "\n",
    "# ttest_sum[:16]\n",
    "\n",
    "# ttest.shape\n",
    "\n",
    "# #how much did each bin contribute?\n",
    "# ttest_sum=ttest.sum(axis=1)\n",
    "# ttest_sum=ttest[:,0]\n",
    "# ttest_shaped=ttest_sum.reshape(pecog.data.shape[0],-1)\n",
    "# cont_bins=ttest_shaped.sum(axis=0)\n",
    "# cont_elecs=ttest_shaped.sum(axis=1)\n",
    "# plt.figure(figsize=(7,3))\n",
    "# plt.bar(np.arange(len(cont_bins)),cont_bins)\n",
    "# ticks=['[0,1]','(1,2]','(2,4]','(4,8]','(8,16]','(16,32]','(32,64]','(64,150]']\n",
    "# plt.xticks(np.arange(len(cont_bins)),ticks)\n",
    "# plt.title('Contributions of Bins to PD 0')\n",
    "# plt.xlabel('Bins')\n",
    "# plt.ylabel('Absolute Contribution')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(3,15))\n",
    "# plt.barh(np.arange(len(cont_elecs)),cont_elecs)\n",
    "# plt.yticks(np.arange(len(cont_elecs)),list(pecog.chan_labels))\n",
    "# plt.ylabel('Region')\n",
    "# plt.xlabel('Absolute Contribution')\n",
    "# plt.title('Contributions of Chans to PD')\n",
    "\n",
    "# mni_file=pd.read_excel('/data2/users/stepeter/mni_coords/cb46fd46/cb46fd46_MNI_atlasRegions.xlsx')\n",
    "# for en,i in enumerate(pecog.chan_labels):\n",
    "#     print(en)\n",
    "#     if( sum(mni_file['Electrode'].isin([i]))==0):\n",
    "#         print(i)\n",
    "#         print(en)\n",
    "        \n",
    "\n",
    "# print(mni_file['Electrode'].loc[7])\n",
    "\n",
    "# pecog.chan_labels.shape\n",
    "\n",
    "# pc1=np.sum(np.abs(ttest[:,1].reshape(-1,8)),axis=1)\n",
    "# pc2=np.sum(np.abs(ttest[:,2].reshape(-1,8)),axis=1)\n",
    "# pc3=np.sum(np.abs(ttest[:,3].reshape(-1,8)),axis=1)\n",
    "# pc4=np.sum(np.abs(ttest[:,4].reshape(-1,8)),axis=1)\n",
    "\n",
    "# mni_coords_fullfile='/data2/users/stepeter/mni_coords/cb46fd46/cb46fd46_MNI_atlasRegions.xlsx'\n",
    "# plot_ecog_electrodes_mni_from_file_and_labels(mni_coords_fullfile,pecog.chan_labels,num_grid_chans=64, colors=cont_elecs[:-1])\n",
    "\n",
    "# plot_ecog_electrodes_mni_from_file_and_labels(mni_coords_fullfile,pecog.chan_labels,num_grid_chans=64, colors=pc1[:-1])\n",
    "# plot_ecog_electrodes_mni_from_file_and_labels(mni_coords_fullfile,pecog.chan_labels,num_grid_chans=64, colors=pc2[:-1])\n",
    "# plot_ecog_electrodes_mni_from_file_and_labels(mni_coords_fullfile,pecog.chan_labels,num_grid_chans=64, colors=pc3[:-1])\n",
    "# plot_ecog_electrodes_mni_from_file_and_labels(mni_coords_fullfile,pecog.chan_labels,num_grid_chans=64, colors=pc4[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
