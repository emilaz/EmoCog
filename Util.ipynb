{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from Evals import get_f1, get_precision_recall, get_f1_from_pr\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, fbeta_score, roc_auc_score, average_precision_score\n",
    "import os\n",
    "import datetime\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationUtils:\n",
    "    \n",
    "#     def get_optimal_threshold(classifier,cv,x,y, go_after_pr = False): #are we optimizing for f1? or tpr-fpr?\n",
    "#         optimal_threshs = []\n",
    "#         for train, test in cv.split(x, y):\n",
    "#             classifier.fit(x[train], y[train])\n",
    "#             probas_ = classifier.predict_proba(x[test])\n",
    "#             # Compute ROC curve\n",
    "#             #this returns different tpr/fpr for different decision thresholds\n",
    "#             if go_after_pr:\n",
    "#                 pre, rec, thresholds = precision_recall_curve(y[test],probas_[:,1])\n",
    "#                 f1 = get_f1_from_pr(pre,rec)\n",
    "#                 optimal_idx = np.argmax(f1)\n",
    "#             else:\n",
    "#                 fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "#                 optimal_idx = np.argmax(tpr - fpr)\n",
    "#             optimal_threshold = thresholds[optimal_idx]\n",
    "#             optimal_threshs.append(optimal_threshold)\n",
    "\n",
    "#         #now that we have this, what was the median best threshold?\n",
    "#         return np.median(optimal_threshs)\n",
    "\n",
    "    \"\"\"\"\"\n",
    "    Finds otimal true probability threshold, depending on either PR curve or ROC curve\n",
    "    Input: Given classifier, sklearn.cv for partitioning, (train) data, desired curve type (PR vs ROC) bool\n",
    "    Output: Optimal threshold for classifying\n",
    "    \"\"\"\n",
    "    def get_optimal_threshold(classifier,cv,x,y, go_after_pr = False): #are we optimizing for f1? or tpr-fpr?\n",
    "        classifier.fit(x,y)\n",
    "        probas_ = classifier.predict_proba(x)\n",
    "        # Compute ROC curve\n",
    "        #this returns different tpr/fpr for different decision thresholds\n",
    "        if go_after_pr:\n",
    "            pre, rec, thresholds = precision_recall_curve(y,probas_[:,1])\n",
    "            f1 = get_f1_from_pr(pre,rec)\n",
    "            optimal_idx = np.argmax(f1)\n",
    "        else:\n",
    "            fpr, tpr, thresholds = roc_curve(y, probas_[:, 1])\n",
    "            optimal_idx = np.argmax(tpr - fpr)\n",
    "            \n",
    "        return thresholds[optimal_idx]\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the median area under curve score, given a cv-partitioning and hyperparas\n",
    "    Input: Classifier, sklearn CV partitioning, data pair, curve type (PR vs. ROC) bool\n",
    "    Output: Median AUC\n",
    "    \"\"\"\n",
    "    def get_auc_score(classifier,cv,x,y, go_after_pr = False): #are we optimizing for f1? or tpr-fpr?\n",
    "        aucs = []\n",
    "        for train, test in cv.split(x, y):\n",
    "            classifier.fit(x[train], y[train])\n",
    "            probas_ = classifier.predict_proba(x[test])\n",
    "            # Compute ROC curve\n",
    "            #this returns different tpr/fpr for different decision thresholds\n",
    "            if go_after_pr:\n",
    "                aucs.append(average_precision_score(y[test],probas_[:,1]))\n",
    "            else:\n",
    "                aucs.append(roc_auc_score(y[test],probas_[:,1]))\n",
    "                \n",
    "        #now that we have this, what was the median best threshold?\n",
    "        return np.median(aucs)\n",
    "\n",
    "    \"\"\"\n",
    "    Given a classifier and data x, return predictions\n",
    "    Input: Sklearn trained classifier, data x, given threshold\n",
    "    Output: Label Predictions\n",
    "    \"\"\"\n",
    "    def get_prediction(classifier,x,thresh):\n",
    "        y_pred = (classifier.predict_proba(x)[:,1]>=thresh).astype(bool)\n",
    "        return y_pred\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits a classifier to given train data and returns predcitions. Optionally also predicts for test data\n",
    "    Input: Classifier, threshold, train data, optional: test dat\n",
    "    \"\"\"\n",
    "    def fit_predict(classifier, thresh, x,y,x_ev=None,y_ev=None):\n",
    "        classifier.fit(x,y)\n",
    "        y_pred = ClassificationUtils.get_prediction(classifier,x,thresh) # predict on whole train set\n",
    "        if x_ev is None:\n",
    "            return y_pred\n",
    "        else:\n",
    "            y_pred_ev = ClassificationUtils.get_prediction(classifier,x_ev,thresh) #same for ev set\n",
    "            return y_pred, y_pred_ev\n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: What is c,g here??\n",
    "    Fills a pd dataframe after a) finding the optimal threshold for a train set, predicting on train and test set and then calculating scores\n",
    "    Input: Dataframe df, dataframe row to fill, Classifier to use, sklearn CV, train data, test data.\n",
    "    \"\"\"\n",
    "    def fit_predict_eval_fill(df,idx,classifier,cv,x,y,x_ev,y_ev):\n",
    "        thresh = ClassificationUtils.get_optimal_threshold(classifier, cv, x, y) # get threshold using cv\n",
    "        y_pred,y_pred_ev = ClassificationUtils.fit_predict(classifier, x, y, x_ev, y_ev, thresh) # using that threshold, get predictions and f1 score\n",
    "        f1_tr=get_f1(y_pred,y) # calculate f1 scores for prediction on train set\n",
    "        f1_ev=get_f1(y_pred_ev,y_ev)\n",
    "        prec_tr,recall_tr = get_precision_recall(y_pred,y)\n",
    "        prec_ev,recall_ev = get_precision_recall(y_pred_ev,y_ev)\n",
    "        results_df.loc[idx] = [c,g,thresh,f1_tr,prec_tr,recall_tr,f1_ev,prec_ev,recall_ev]\n",
    "        \n",
    "    \"\"\"\n",
    "    Gives you the row with the best results, given a column specifying which metric to judge by (F1 or other)\n",
    "    Input: Datafrane, Column\n",
    "    Output: Row with best results\n",
    "    \"\"\"\n",
    "    def get_best_hyperparas_results(df, col):\n",
    "        pos = df[col].idxmax()\n",
    "        best_row=df.loc[pos] # get the row with highest ev score\n",
    "        return best_row       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class is mainly concerned with saving and loading data. \n",
    "It also includes a method to set the usual hyperparameters for loading data faster (no need to calculate them again)\n",
    "\"\"\"\n",
    "class DataUtils:\n",
    "\n",
    "    def load_configs():\n",
    "        configs = dict()\n",
    "        configs['sliding'] = False\n",
    "        configs['wsize'] = 5\n",
    "        configs['s_sample']= 0\n",
    "        configs['e_sample']= 30000\n",
    "        configs['s_sample_ev'] = 30000\n",
    "        configs['e_sample_ev'] = 35000\n",
    "        configs['cutoff'] = .2\n",
    "        return configs\n",
    "    \"\"\"\n",
    "    Generates a filename out of the configs\n",
    "    Input: Configs\n",
    "    Output: Filename\n",
    "    \"\"\"\n",
    "    def generate_filename(configs):\n",
    "        fname = 'ws_'+str(configs['wsize'])+'_str_'+str(configs['sliding'])+'_tr'+'_s_'+str(configs['s_sample'])+'_e_'+str(configs['e_sample'])+'_ev_'+'s_'+str(configs['s_sample_ev'])+'_e_'+str(configs['e_sample_ev'])+'_expvar_'+str(configs['expvar'])\n",
    "        return fname\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates configs from a filename\n",
    "    Input: Filename\n",
    "    Output: Configs\n",
    "    \"\"\"\n",
    "    def generate_configs_from_file(file,cutoff=None):\n",
    "        configs = dict()\n",
    "        underscores = [pos for pos, char in enumerate(file) if char == '_']\n",
    "        configs['wsize'] = int(file[underscores[0]+1:underscores[1]])\n",
    "        try:\n",
    "            configs['sliding'] = int(file[underscores[2]+1:underscores[3]])\n",
    "        except ValueError: #not sure if 0 isn't leading to a sliding window with stride 0, hence this safety measure\n",
    "            configs['sliding'] = False\n",
    "        configs['s_sample'] = int(file[underscores[5]+1:underscores[6]])\n",
    "        configs['e_sample'] = int(file[underscores[7]+1:underscores[8]])\n",
    "        configs['s_sample_ev'] = int(file[underscores[10]+1:underscores[11]])\n",
    "        configs['e_sample_ev'] = int(file[underscores[12]+1:underscores[13]])\n",
    "        configs['expvar'] = int(file[underscores[14]+1:underscores[14]+3])\n",
    "        if cutoff:\n",
    "            configs['cutoff'] = float(cutoff)\n",
    "        return configs\n",
    "           \n",
    "    \"\"\"\n",
    "    This method loads the data (train and test) into memory, given the configs. Obviously only works when data for these configs has been generated and saved some previous time\n",
    "    Input: Configs\n",
    "    Output: Train and test data according to these configs\n",
    "    \"\"\"\n",
    "    def load_data_from_file(configs):\n",
    "        fname = DataUtils.generate_filename(configs)\n",
    "        link = '/home/emil/OpenMindv2/data/several_days/'+fname+'.hdf'\n",
    "        df = pd.read_hdf(link)\n",
    "        x = df['x'][0]\n",
    "        y = df['y'][0]\n",
    "        x_ev = df['x_ev'][0]\n",
    "        y_ev = df['y_ev'][0]\n",
    "        return x,y,x_ev,y_ev\n",
    "    \n",
    "    \"\"\"\n",
    "    This method saves the generated train and test data to a file, given the corresponding configs. Automatically generates an appropriate filename.\n",
    "    Input: Train and test data, configs\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    def save_data_to_file(x,y,x_ev,y_ev,configs):\n",
    "        fname = DataUtils.generate_filename(configs)\n",
    "        link = '/home/emil/OpenMindv2/data/several_days/'+fname+'.hdf'\n",
    "        #save stuff to file:\n",
    "        df = pd.DataFrame(data=[[x,y,x_ev,y_ev]],columns=['x','y','x_ev','y_ev'])\n",
    "        df.to_hdf(link,key='df')\n",
    "        \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    This methods loads data given a file path\n",
    "    Input: File path\n",
    "    Output: x,y,x_ev,y_ev\n",
    "    \"\"\"\n",
    "    def load_data_from_path(path):\n",
    "        df = pd.read_hdf(path)\n",
    "        x = df['x'][0]\n",
    "        y = df['y'][0]\n",
    "        x_ev = df['x_ev'][0]\n",
    "        y_ev = df['y_ev'][0]\n",
    "        return x,y,x_ev,y_ev\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    This method saves the obtained results to a file for later inspection.\n",
    "    Input: Datafrme with results, config file, ML approach used\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    def save_results(df, configs,methodtype):\n",
    "        fname = DataUtils.generate_filename(configs)\n",
    "        fname = fname +'_cut_'+str(configs['cutoff']) #cutoff info is important for results\n",
    "        link = '/home/emil/OpenMindv2/data/results/several_days/'+fname+methodtype\n",
    "        df.to_hdf(link,key='df')\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe with results, given the configs and the ML-method type in question\n",
    "    Input: Configs, Method\n",
    "    Output: Dataframe with results\n",
    "    \"\"\"\n",
    "    def load_results(configs,methodtype):\n",
    "        fname = DataUtils.generate_filename(configs)\n",
    "        fname = fname +'_cut_'+str(configs['cutoff'])\n",
    "        link = '/home/emil/OpenMindv2/data/results/several_days/'+fname+methodtype\n",
    "        df = pd.read_hdf(link)\n",
    "        return df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureUtils:\n",
    "    \n",
    "    \"\"\"\n",
    "    Standardizes data (demean& unit variance)\n",
    "    Input: Data, standart deviation of data, mean of data\n",
    "    \"\"\"\n",
    "    def standardize(data,std,data_mean):\n",
    "        data_dem = data-data_mean[:,None]\n",
    "        data_stand = data_dem/(std[:,None])\n",
    "        return data_stand\n",
    "    \n",
    "    \"\"\"\n",
    "    Return indices of bad time points, based on outlier detection\n",
    "    Input: Data matrix\n",
    "    Output: Bad indices matrix, same length as data matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect_artifacts(data_matrix, std_lim=None, med_lim = None):\n",
    "        #first, we only want to look at the high-freq bin:\n",
    "        high_freqs = data_matrix[7::8,:]\n",
    "        #next, calculate median std across each channel(should be 1 I think)\n",
    "        if std_lim is None:\n",
    "            std_lim = np.std(high_freqs,axis=1)\n",
    "            med_lim = np.median(high_freqs,axis=1)\n",
    "        #next, for each row, get all indices that are too far away from the median in one direction\n",
    "        too_high = high_freqs>(med_lim+3*std_lim)[:,None] #should yield a 2D matrix\n",
    "        #for each column, check if there is an entry that's too high\n",
    "        too_high_idx = np.any(too_high,axis=0)\n",
    "        #same for other direction\n",
    "        too_low = high_freqs<(med_lim-3*std_lim)[:,None] #should yield a 2D matrix\n",
    "        too_low_idx = np.any(too_low,axis=0)\n",
    "        bad_idx = too_low_idx | too_high_idx #any index either too high or too low? that is a bad index\n",
    "        return bad_idx, std_lim, med_lim\n",
    "    \n",
    "    \n",
    "    def remove_artifacts(data, bad_indices):\n",
    "        good_data = data[:,~bad_indices] #keep only the good indices\n",
    "        return good_data\n",
    "        \n",
    "\n",
    "    #this function caps at 150Hz, then bins the data in a logarithmic fashion to account for smaller psd values in higher freqs\n",
    "    \"\"\"\n",
    "    Key function for feature processing. Truncates frequencies above 150Hz, bins the frequencies logarithmically.\n",
    "    Throws the PSD into these bins by summing all PSD that fall into a certain bin.\n",
    "    Input: Frequency array, PSD array\n",
    "    Output: Binned frequencies, binned PSD\n",
    "    \"\"\"\n",
    "    def bin_psd(fr,psd):\n",
    "        fr_trun=fr[fr<=150]\n",
    "        fr_total=len(fr_trun)\n",
    "        fr_bins=np.arange(int(np.log2(max(fr_trun))+1))\n",
    "        #truncate everythin above 150Hz\n",
    "        psd=psd[:,fr<=150]\n",
    "        psd_bins=np.zeros((psd.shape[0],len(fr_bins)))\n",
    "        prev=0\n",
    "        #these are the general upper limits. they don't give info where in fr/psd these frequencies acutally are!\n",
    "        max_psd_per_bin=np.exp2(fr_bins).astype('int')\n",
    "        #hence we need this method. It gives us a 2D array with the number of rows = number of bins,\n",
    "        #where each row contains 2 values, upper and lower limit of that bin\n",
    "        prev=0\n",
    "        limits=np.zeros((max_psd_per_bin.shape[0],2),dtype='int')\n",
    "        for en,b in enumerate(max_psd_per_bin):\n",
    "            if en==0:\n",
    "                arr=np.where((fr_trun >=prev)&(fr_trun<=b))[0]\n",
    "            else:\n",
    "                arr=np.where((fr_trun >prev)&(fr_trun<=b))[0]\n",
    "            check=np.array([min(arr),max(arr)])\n",
    "            limits[np.log2(b).astype('int')]=check\n",
    "            prev=b\n",
    "        prev=0\n",
    "        #now, we will the psd_bins by adding up all psd in the range defined before. The last bin gets all remaining frequencies\n",
    "        #meaning, if the last bin ranges from 64-128, it'll also include everything until 150.\n",
    "        for b in fr_bins:\n",
    "            if (b==fr_bins[-1] or limits[b][1]>=fr_total):\n",
    "                psd_bins[:,b]+=np.sum(psd[:,limits[b,0]:],axis=1)\n",
    "            else:\n",
    "                psd_bins[:,b]=np.sum(psd[:,limits[b,0]:limits[b,1]+1],axis=1)\n",
    "        return fr_bins, psd_bins\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Function for PCA.\n",
    "    Given some minimum of explained variance of the data, return the number of components needed (at most 100).\n",
    "    Input: Data, desired variance explained\n",
    "    Output: Number of Components needed.\n",
    "    \"\"\"\n",
    "    def get_no_comps(data,expl_var_lim):\n",
    "        comps=min(100,min(data.shape))\n",
    "        pca=PCA(n_components=comps)\n",
    "        pca.fit(data)\n",
    "        tot=0\n",
    "        for idx,c in enumerate(pca.explained_variance_ratio_):\n",
    "            tot+=c\n",
    "            if tot*100>expl_var_lim:\n",
    "                return idx+1\n",
    "        return pca.n_components_\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to find the indices of channels that were good across days.\n",
    "    Input: PD Series with good channel info per day\n",
    "    Output: Good Indices?\n",
    "    \"\"\"\n",
    "    def find_common_channels(channels_per_day):\n",
    "        common = reduce(np.intersect1d, channels_per_day['GoodChans']) #which channels are good on all days?\n",
    "        #get the indices of these common channels, for each day\n",
    "        good_df = pd.DataFrame(columns = ['Day','CommonChans'])\n",
    "        for idx, row in channels_per_day.iterrows():\n",
    "            indices=[np.min(np.nonzero(row['GoodChans']==ch)[0]) for ch in common] #these are the indices to keep - not necessarily in correct order!\n",
    "            good = np.zeros(len(row['GoodChans']),dtype='bool') #therefore, bool array for order\n",
    "            good[indices] = True\n",
    "            good_df.loc[idx] = [row['Day'], good]\n",
    "        return good_df\n",
    "    \n",
    "    \n",
    "    def filter_common_channels(common_df):\n",
    "        good_idx = FeatureUtils.find_common_channels(common_df[['Day','GoodChans']])\n",
    "        ret_df = common_df.copy()\n",
    "        for idx,day in enumerate(good_idx['Day']):\n",
    "            good = good_idx.loc[good_idx['Day']==day,'CommonChans'][idx]\n",
    "            new_data = common_df[common_df['Day']==day]['BinnedData'][idx][good,:,:]\n",
    "            ret_df.loc[common_df['Day']==day,'BinnedData'] = [new_data]\n",
    "        return ret_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelUtils:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Converts strings into int/np.nan. Necessary for if looking at annotations in dataframe\n",
    "    Input: Data\n",
    "    Output: Mal gukcen\n",
    "    \"\"\"\n",
    "    def convert_labels_readable(annot):\n",
    "        nan_indices= annot=='N/A'\n",
    "        annot[annot!='Happy']=0\n",
    "        annot[annot=='Happy']=1\n",
    "        annot[nan_indices]=np.nan\n",
    "        return annot\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Converts labels in range [0,1] into binary Happy/Not Happy, given cutoff\n",
    "    Input: Labels, cutoff\n",
    "    Output: Binary Labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def do_cutoff(y_in,cutoff):\n",
    "        y = y_in.copy()\n",
    "        y[y>cutoff] = 1\n",
    "        y[y<1] = 0\n",
    "        return y\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    For a given video, finds the number of frames the video is supposed to have.\n",
    "    Input: The hdf dataframe from Gautham's network, the given video\n",
    "    Output: Number of frames the video should have\n",
    "    \"\"\"\n",
    "    \n",
    "    def find_number_frames(df, vid, last_vid, fill_vid_after):\n",
    "        next_vid = str(int(vid)+1) #which is the next number?\n",
    "        len_vid = len(next_vid)\n",
    "        next_vid = '0'*(4-len_vid)+next_vid\n",
    "        time_vid = df[df['vid']==vid]['datetime'].iloc[0]\n",
    "        try:\n",
    "            time_next_vid = df[df['vid']==next_vid]['datetime'].iloc[0]\n",
    "            time = datetime.datetime.strptime(time_vid, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            new_time = datetime.datetime.strptime(time_next_vid, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            elapsed = (new_time-time).total_seconds()\n",
    "        except: #in case of last video\n",
    "            if last_vid != vid:\n",
    "                fill_vid_after[0] = True\n",
    "            elapsed = 120\n",
    "        elapsed_frames = int(elapsed *30)\n",
    "        return elapsed_frames\n",
    "    \n",
    "    \"\"\"\n",
    "    This function distributes the given m frames along an array of size n (>m), the rest are nans\n",
    "    Input: The given labels for some video, the number of frames(and therefore labels) the video should have\n",
    "    Output: Array of length n with frames distributed along array (in ordered manner!)\"\"\"\n",
    "\n",
    "    def fill_frames(actual_labels,supposed_no_labels):\n",
    "        no_labels = len(actual_labels) #how many labels do we have?\n",
    "        places = np.empty(supposed_no_labels) #create array of length we actually want\n",
    "        places[:]=np.nan\n",
    "        positions = sorted(np.random.choice(supposed_no_labels,no_labels,replace=False)) #which positions do we want to fill?\n",
    "        places[positions]=actual_labels\n",
    "        return places        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyncUtils:\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Given a day, finds the corresponding video file I guess\n",
    "    \"\"\"\n",
    "    #TODO: Check if session is longer than a day\n",
    "    def find_paths(patient_name,day_no):\n",
    "        #create filename:\n",
    "        filename = patient_name+'_fullday_'+str(day_no)+'.h5'\n",
    "        path = os.path.join('/nas/ecog_project/derived/processed_ecog',patient_name,'full_day_ecog')\n",
    "        ecog_path = os.path.join(path, filename)\n",
    "        #check if file exists\n",
    "        if not os.path.exists(ecog_path):\n",
    "            print('ECoG File does not exist. Check if day or patient name is wrong.')\n",
    "            return\n",
    "        else: #ok nice, now get video paths\n",
    "            #read in the csv file. also includes the corresponding ecog file\n",
    "            vid_info = os.path.join(path, 'vid_start_end_merge.csv')\n",
    "            vid_csv =pd.read_csv(vid_info)\n",
    "            vid_relevant = vid_csv[vid_csv['merge_day']==int(day_no)]\n",
    "            #this is a bit hacky. To get the video session that fits into the given day\n",
    "            try:\n",
    "                sess_start = vid_relevant[vid_relevant['filename'].str.contains('0000')].iloc[0]\n",
    "            except:\n",
    "                FileNotFoundError('No session start for this day. Something might be wrong in SyncUtils.')\n",
    "            #what is the session name?\n",
    "            sess = sess_start['filename'].split('_')[1]\n",
    "            #we want something akin to this format: cb46fd46_5_imp_columns\n",
    "            vid_filename = '_'.join([patient_name,sess,'imp','columns.hdf'])\n",
    "            vid_path = os.path.join('/home/emil/data/hdf_data',vid_filename)\n",
    "            #check if gautham file exists\n",
    "            if not os.path.exists(vid_path):\n",
    "                print('ECoG File does not exist. Check if day or patient name is wrong.')\n",
    "                return\n",
    "            return ecog_path, vid_path\n",
    "        \n",
    "    \"\"\"\n",
    "    This function finds out how many seconds into the day the given session start\n",
    "    Input: Path to video session hdf file\n",
    "    Output: Seconds passed since midnight until video session began\"\"\"\n",
    "    def find_start_and_end_time(vid_path):\n",
    "        #first_frame = pd.read_hdf(vid_path, stop = 1) #read only first line of hdf file\n",
    "        store = pd.HDFStore(vid_path)\n",
    "        #first, where is the start of the video, in secs?\n",
    "        start_time = store.select('df',stop =1)['realtime'].iloc[0]\n",
    "        start_in_secs = (start_time - start_time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds()\n",
    "        #now for the end of the video. First, check if still same day\n",
    "        nrows = store.get_storer('df').shape[0]    \n",
    "        end_time = store.select('df', start = nrows -1, stop = nrows)['realtime'].iloc[0]\n",
    "        if end_time.date() == start_time.date():\n",
    "            end_in_secs = (end_time - end_time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds()\n",
    "        else: #if the file goes well into the next day, then we'll just everything until midnight\n",
    "            print('Current session goes into the next day, so edge case. Check if things went correctly.')\n",
    "            end_in_secs = 24*3600\n",
    "        store.close()\n",
    "        return int(start_in_secs), int(end_in_secs)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to filter bad/faulty data points. The order is important, since (esp. bad feature points) indices are relative to previous filterings\n",
    "    Input: x,y, indices\n",
    "    Output: x,y filtered\n",
    "    \"\"\"\n",
    "    \n",
    "    def filter_data(x,y,feat_ind):\n",
    "        #first, filter the NaNs on feature side out on label side as well\n",
    "        bad_feat_nan = np.array(feat_ind['NaNs'])\n",
    "        print(bad_feat_nan, 'indices to kill')\n",
    "        bad_feat_nan_inbounds = bad_feat_nan[bad_feat_nan<len(y)] #this shouldn't do anything I think\n",
    "        if(len(bad_feat_nan)>len(bad_feat_nan_inbounds)):\n",
    "            print('jo diese laengen solltten gleich sein.')\n",
    "        y_no_feat_nans = np.delete(y,bad_feat_nan)\n",
    "        #now the artifacts\n",
    "        bad_feat_artifacts = feat_ind['Artifacts'] \n",
    "        bad_feat_artifacts_inbounds = bad_feat_artifacts[bad_feat_artifacts<len(y_no_feat_nans)]\n",
    "        if(len(bad_feat_artifacts)<len(bad_feat_artifacts_inbounds)):\n",
    "            print('jo diese laengen sollten auch gleich sein.')\n",
    "        y_no_arts = y_no_feat_nans[~bad_feat_artifacts_inbounds]\n",
    "        #finally, filter out the nans on label side:\n",
    "        x_ret = x[:,~np.isnan(y_no_arts)]\n",
    "        y_ret = y_no_arts[~np.isnan(y_no_arts)]\n",
    "        return x_ret, y_ret\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86400"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24*3600"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P3 emocog",
   "language": "python",
   "name": "emocog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
