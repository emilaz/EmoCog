{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from Evals import get_f1, get_precision_recall, get_f1_from_pr\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, fbeta_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationUtils:\n",
    "    \n",
    "#     def get_optimal_threshold(classifier,cv,x,y, go_after_pr = False): #are we optimizing for f1? or tpr-fpr?\n",
    "#         optimal_threshs = []\n",
    "#         for train, test in cv.split(x, y):\n",
    "#             classifier.fit(x[train], y[train])\n",
    "#             probas_ = classifier.predict_proba(x[test])\n",
    "#             # Compute ROC curve\n",
    "#             #this returns different tpr/fpr for different decision thresholds\n",
    "#             if go_after_pr:\n",
    "#                 pre, rec, thresholds = precision_recall_curve(y[test],probas_[:,1])\n",
    "#                 f1 = get_f1_from_pr(pre,rec)\n",
    "#                 optimal_idx = np.argmax(f1)\n",
    "#             else:\n",
    "#                 fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "#                 optimal_idx = np.argmax(tpr - fpr)\n",
    "#             optimal_threshold = thresholds[optimal_idx]\n",
    "#             optimal_threshs.append(optimal_threshold)\n",
    "\n",
    "#         #now that we have this, what was the median best threshold?\n",
    "#         return np.median(optimal_threshs)\n",
    "\n",
    "    \"\"\"\"\"\n",
    "    Finds otimal true probability threshold, depending on either PR curve or ROC curve\n",
    "    Input: Given classifier, sklearn.cv for partitioning, (train) data, desired curve type (PR vs ROC) bool\n",
    "    Output: Optimal threshold for classifying\n",
    "    \"\"\"\n",
    "    def get_optimal_threshold(classifier,cv,x,y, go_after_pr = False): #are we optimizing for f1? or tpr-fpr?\n",
    "        classifier.fit(x,y)\n",
    "        probas_ = classifier.predict_proba(x)\n",
    "        # Compute ROC curve\n",
    "        #this returns different tpr/fpr for different decision thresholds\n",
    "        if go_after_pr:\n",
    "            pre, rec, thresholds = precision_recall_curve(y,probas_[:,1])\n",
    "            f1 = get_f1_from_pr(pre,rec)\n",
    "            optimal_idx = np.argmax(f1)\n",
    "        else:\n",
    "            fpr, tpr, thresholds = roc_curve(y, probas_[:, 1])\n",
    "            optimal_idx = np.argmax(tpr - fpr)\n",
    "            \n",
    "        return thresholds[optimal_idx]\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the mean area under curve score for a given  cv-partitioning\n",
    "    Input: Classifier, sklearn CV partitioning, data pair, curve type (PR vs. ROC) bool\n",
    "    Output: Mean AUC\n",
    "    \"\"\"\n",
    "    def get_auc_score(classifier,cv,x,y, go_after_pr = False): #are we optimizing for f1? or tpr-fpr?\n",
    "        aucs = []\n",
    "        for train, test in cv.split(x, y):\n",
    "            classifier.fit(x[train], y[train])\n",
    "            probas_ = classifier.predict_proba(x[test])\n",
    "            # Compute ROC curve\n",
    "            #this returns different tpr/fpr for different decision thresholds\n",
    "            if go_after_pr:\n",
    "                aucs.append(average_precision_score(y[test],probas_[:,1]))\n",
    "            else:\n",
    "                aucs.append(roc_auc_score(y[test],probas_[:,1]))\n",
    "                \n",
    "        #now that we have this, what was the median best threshold?\n",
    "        return np.mean(aucs)\n",
    "\n",
    "    \"\"\"\n",
    "    Given a classifier and data x, return predictions\n",
    "    Input: Sklearn trained classifier, data x, given threshold\n",
    "    Output: Label Predictions\n",
    "    \"\"\"\n",
    "    def get_prediction(classifier,x,thresh):\n",
    "        y_pred = (classifier.predict_proba(x)[:,1]>=thresh).astype(bool)\n",
    "        return y_pred\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits a classifier to given train data and returns predcitions. Optionally also predicts for test data\n",
    "    Input: Classifier, threshold, train data, optional: test dat\n",
    "    \"\"\"\n",
    "    def fit_predict(classifier, thresh, x,y,x_ev=None,y_ev=None):\n",
    "        classifier.fit(x,y)\n",
    "        y_pred = ClassificationUtils.get_prediction(classifier,x,thresh) # predict on whole train set\n",
    "        if x_ev is None:\n",
    "            return y_pred\n",
    "        else:\n",
    "            y_pred_ev = ClassificationUtils.get_prediction(classifier,x_ev,thresh) #same for ev set\n",
    "            return y_pred, y_pred_ev\n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: What is c,g here??\n",
    "    Fills a pd dataframe after a) finding the optimal threshold for a train set, predicting on train and test set and then calculating scores\n",
    "    Input: Dataframe df, dataframe row to fill, Classifier to use, sklearn CV, train data, test data.\n",
    "    \"\"\"\n",
    "    def fit_predict_eval_fill(df,idx,classifier,cv,x,y,x_ev,y_ev):\n",
    "        thresh = ClassificationUtils.get_optimal_threshold(classifier, cv, x, y) # get threshold using cv\n",
    "        y_pred,y_pred_ev = ClassificationUtils.fit_predict(classifier, x, y, x_ev, y_ev, thresh) # using that threshold, get predictions and f1 score\n",
    "        f1_tr=get_f1(y_pred,y) # calculate f1 scores for prediction on train set\n",
    "        f1_ev=get_f1(y_pred_ev,y_ev)\n",
    "        prec_tr,recall_tr = get_precision_recall(y_pred,y)\n",
    "        prec_ev,recall_ev = get_precision_recall(y_pred_ev,y_ev)\n",
    "        results_df.loc[idx] = [c,g,thresh,f1_tr,prec_tr,recall_tr,f1_ev,prec_ev,recall_ev]\n",
    "        \n",
    "    \"\"\"\n",
    "    Gives you the row with the best results, given a column specifying which metric to judge by (F1 or other)\n",
    "    Input: Datafrane, Column\n",
    "    Output: Row with best results\n",
    "    \"\"\"\n",
    "    def get_best_hyperparas_results(df, col):\n",
    "        pos = df[col].idxmax()\n",
    "        best_row=df.loc[pos] # get the row with highest ev score\n",
    "        return best_row       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class is mainly concerned with saving and loading data. \n",
    "It also includes a method to set the usual hyperparameters for loading data faster (no need to calculate them again)\n",
    "\"\"\"\n",
    "class DataUtils:\n",
    "\n",
    "    def load_configs():\n",
    "        configs = dict()\n",
    "        configs['sliding'] = 10\n",
    "        configs['wsize'] = 100\n",
    "        configs['s_sample']= 0\n",
    "        configs['e_sample']= 30000\n",
    "        configs['s_sample_ev'] = 30000\n",
    "        configs['e_sample_ev'] = 35000\n",
    "        configs['cutoff'] = .2\n",
    "        return configs\n",
    "    \"\"\"\n",
    "    Generates a filename out of the configs\n",
    "    Input: Configs\n",
    "    Output: Filename\n",
    "    \"\"\"\n",
    "    def generate_filename(configs):\n",
    "        fname = 'ws_'+str(configs['wsize'])+'_str_'+str(configs['sliding'])+'_tr'+'_s_'+str(configs['s_sample'])+'_e_'+str(configs['e_sample'])+'_ev_'+'s_'+str(configs['s_sample_ev'])+'_e_'+str(configs['e_sample_ev'])+'_cut_'+str(configs['cutoff'])\n",
    "        return fname\n",
    "    \n",
    "    \"\"\"\n",
    "    This method loads the data (train and test) into memory, given the configs. Obviously only works when data for these configs has been generated and saved some previous time\n",
    "    Input: Configs\n",
    "    Output: Train and test data according to these configs\n",
    "    \"\"\"\n",
    "    def get_data_from_file(configs):\n",
    "        fname = DataUtils.generate_filename(configs)\n",
    "        link = '/home/emil/OpenMindv2/data/'+fname+'.hdf'\n",
    "        df = pd.read_hdf(link)\n",
    "        x = df['x'][0]\n",
    "        y = df['y'][0]\n",
    "        x_ev = df['x_ev'][0]\n",
    "        y_ev = df['y_ev'][0]\n",
    "        return x,y,x_ev,y_ev\n",
    "    \n",
    "    \"\"\"\n",
    "    This method saves the generated train and test data to a file, given the corresponding configs. Automatically generates an appropriate filename.\n",
    "    Input: Train and test data, configs\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    def save_data_to_file(x,y,x_ev,y_ev,configs):\n",
    "        fname = DataUtils.generate_filename(configs)\n",
    "        link = '/home/emil/OpenMindv2/data/'+fname+'.hdf'\n",
    "        #save stuff to file:\n",
    "        df = pd.DataFrame(data=[[x,y,x_ev,y_ev]],columns=['x','y','x_ev','y_ev'])\n",
    "        df.to_hdf(link,key='df')\n",
    "        \n",
    "    \"\"\"\n",
    "    This method saves the obtained results to a file for later inspection.\n",
    "    Input: Datafrme with results, config file, ML approach used\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    def save_results(df, configs,methodtype):\n",
    "        fname = DataUtils.generate_filename(configs)\n",
    "        link = '/home/emil/OpenMindv2/data/results/'+fname+methodtype\n",
    "        df.to_hdf(link,key='df')\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe with results, given the configs and the ML-method type in question\n",
    "    Input: Configs, Method\n",
    "    Output: Dataframe with results\n",
    "    \"\"\"\n",
    "    def get_results(configs,methodtype):\n",
    "        fname = DataUtils.generate_filename(configs)\n",
    "        link = '/home/emil/OpenMindv2/data/results/'+fname+methodtype\n",
    "        df = pd.read_hdf(link)\n",
    "        return df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureUtils:\n",
    "    \n",
    "    \"\"\"\n",
    "    Standardizes data (demean& unit variance)\n",
    "    Input: Data, standart deviation of data, mean of data\n",
    "    \"\"\"\n",
    "    def standardize(data,std,data_mean):\n",
    "        #data_mean = np.mean(data,axis=ax)\n",
    "        data_dem = data-data_mean\n",
    "        #std = np.std(data,axis=ax)\n",
    "        data_stand = data_dem/std\n",
    "        return data_stand\n",
    "\n",
    "    #this function caps at 150Hz, then bins the data in a logarithmic fashion to account for smaller psd values in higher freqs\n",
    "    \"\"\"\n",
    "    Key function for feature processing. Truncates frequencies above 150Hz, bins the frequencies logarithmically.\n",
    "    Throws the PSD into these bins by summing all PSD that fall into a certain bin.\n",
    "    Input: Frequency array, PSD array\n",
    "    Output: Binned frequencies, binned PSD\n",
    "    \"\"\"\n",
    "    def bin_psd(fr,psd):\n",
    "        fr_trun=fr[fr<=150]\n",
    "        fr_total=len(fr_trun)\n",
    "        fr_bins=np.arange(int(np.log2(max(fr_trun))+1))\n",
    "        #truncate everythin above 150Hz\n",
    "        psd=psd[:,fr<=150]\n",
    "        psd_bins=np.zeros((psd.shape[0],len(fr_bins)))\n",
    "        prev=0\n",
    "        #these are the general upper limits. they don't give info where in fr/psd these frequencies acutally are!\n",
    "        max_psd_per_bin=np.exp2(fr_bins).astype('int')\n",
    "        #hence we need this method:\n",
    "        prev=0\n",
    "        limits=np.zeros((max_psd_per_bin.shape[0],2),dtype='int')\n",
    "        for en,b in enumerate(max_psd_per_bin):\n",
    "            if en==0:\n",
    "                arr=np.where((fr_trun >=prev)&(fr_trun<=b))[0]\n",
    "            else:\n",
    "                arr=np.where((fr_trun >prev)&(fr_trun<=b))[0]\n",
    "            check=np.array([min(arr),max(arr)])\n",
    "            limits[np.log2(b).astype('int')]=check\n",
    "            prev=b\n",
    "        prev=0\n",
    "        for b in fr_bins:\n",
    "            if (b==fr_bins[-1] or limits[b][1]>=fr_total):\n",
    "                psd_bins[:,b]+=np.sum(psd[:,limits[b,0]:],axis=1)\n",
    "            else:\n",
    "                psd_bins[:,b]=np.sum(psd[:,limits[b,0]:limits[b,1]+1],axis=1)\n",
    "        return fr_bins, psd_bins\n",
    "\n",
    "    \"\"\"\n",
    "    Function for PCA.\n",
    "    Given some minimum of explained variance of the data, return the number of components needed (at most 100).\n",
    "    Input: Data, desired variance explained\n",
    "    Output: Number of Components needed.\"\"\"\n",
    "    def get_no_comps(data,expl_var_lim):\n",
    "        comps=min(100,min(data.shape))\n",
    "        pca=PCA(n_components=comps)\n",
    "        pca.fit(data)\n",
    "        tot=0\n",
    "        for idx,c in enumerate(pca.explained_variance_ratio_):\n",
    "            tot+=c\n",
    "            if tot*100>expl_var_lim:\n",
    "                return idx+1\n",
    "        return pca.n_components_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P3 emocog",
   "language": "python",
   "name": "emocog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
